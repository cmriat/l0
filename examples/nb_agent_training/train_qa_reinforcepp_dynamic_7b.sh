VERL_LOGGING_LEVEL=INFO PYTHONUNBUFFERED=1 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python3 examples/nb_agent_training/train_nb_agent.py \
 data.train_files=./data/train.parquet \
 data.val_files=./data/validation.parquet \
 data.train_batch_size=64 \
 data.val_batch_size=128 \
 data.max_prompt_length=16384 \
 data.max_response_length=3072 \
 actor_rollout_ref.traj_sampler.remote_exec_server_url=['http://your_server_ip:8000'] \
 actor_rollout_ref.traj_sampler.executor_type=remote \
 actor_rollout_ref.traj_sampler.agent.task_run.max_steps=8 \
 actor_rollout_ref.traj_sampler.max_concurrency=32 \
 actor_rollout_ref.traj_sampler.task_timeout=240 \
 actor_rollout_ref.traj_sampler.max_retries=1 \
 actor_rollout_ref.traj_sampler.agent.nb_agent.traj_save_storage_path=./output/trajectories \
 actor_rollout_ref.traj_sampler.agent.log_path=./output/logs \
 actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
 actor_rollout_ref.model.use_remove_padding=True \
 actor_rollout_ref.actor.optim.lr=1e-6 \
 actor_rollout_ref.rollout.n=4 \
 actor_rollout_ref.actor.do_not_split_ppo_mini_batch=True \
 actor_rollout_ref.actor.use_kl_loss=True \
 actor_rollout_ref.actor.fsdp_config.param_offload=True \
 actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
 actor_rollout_ref.actor.use_dynamic_bsz=True \
 actor_rollout_ref.actor.ppo_max_token_len_per_gpu=40000 \
 actor_rollout_ref.rollout.name=sglang \
 actor_rollout_ref.rollout.val_kwargs.n=1 \
 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=2 \
 actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
 actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=2 \
 reward_model.reward_manager=multistep \
 algorithm.token_level_adv_estimator=trivial \
 algorithm.step_level_adv_estimator=reinforce_plus_plus \
 algorithm.filter_groups.enable=True \
 algorithm.filter_groups.max_num_gen_batches=3 \
 trainer.logger=['wandb, console'] \
 trainer.val_before_train=False \
 trainer.default_hdfs_dir=null \
 trainer.n_gpus_per_node=8 \
 trainer.nnodes=2 \
 trainer.save_freq=10 \
 trainer.test_freq=50 \
 trainer.experiment_name=l0-7b \
 trainer.project_name='l0' \
 trainer.total_epochs=2 2>&1 | tee ./output/train.log
